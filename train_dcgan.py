import torch
import torch.nn as nn
import torchvision.utils as vutils

from trainer import GanTrainer
from models import DCGAN
from dataset import CelebA as Dataset

from utils.functions import save_pic, print_and_log, sys_flush_log

import numpy as np
import matplotlib.pyplot as plt

trainer = GanTrainer("DCGAN_CelebA")

z_dim = 100
num_gen_feature = 64
num_dis_feature = 64
num_channel = 3
plt_iters = 100
g_lr = 0.0002
d_lr = 0.0002

# Beta1 hyperparam for Adam optimizers
beta1 = 0.5

if __name__ == '__main__':
    args = trainer.args
    device = trainer.device

    G, D = DCGAN(device)
    dataloader = Dataset(batch_size=args.batch_size, num_worker=args.num_worker, resize=64)

    criterion = nn.BCELoss()  # 需不需要 to(device) 是个问题
    optimizerG = torch.optim.Adam(G.parameters(), lr=g_lr, betas=(beta1, 0.999))
    optimizerD = torch.optim.Adam(D.parameters(), lr=d_lr, betas=(beta1, 0.999))

    fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)

    G_losses = []
    D_losses = []
    iters = 0

    for epoch in range(args.epochs):
        running_g_loss = 0.0
        running_d_loss = 0.0
        for i, data in enumerate(dataloader, 0):
            D.zero_grad()
            real_image = data[0].to(device)
            batch_size = real_image.size(0)
            label = torch.full((batch_size,), 1., dtype=torch.float, device=device)

            # 使用真实数据训练鉴别器
            output = D(real_image).view(-1)
            lossD_real = criterion(output, label)
            lossD_real.backward()
            D_x = output.mean().item()  # 临时保存 D(x)-对正确的数据的判别结果

            # 使用噪声生成假的图像数据
            label.fill_(0.)
            noise = torch.randn(batch_size, z_dim, 1, 1, device=device)
            fake_image = G(noise)

            # 使用假的数据训练鉴别器
            output = D(fake_image.detach()).view(-1)
            lossD_fake = criterion(output, label)
            lossD_fake.backward()
            lossD = lossD_real + lossD_fake  # 计算 D 的全部损失
            optimizerD.step()
            D_G_z1 = output.mean().item()  # 临时保存 D(G(z))-对错误的数据的判别结果

            # 对判别器进行训练
            G.zero_grad()
            label.fill_(1.)  # fake labels are real for generator cost
            # Since we just updated D, perform another forward pass of all-fake batch through D
            output = D(fake_image).view(-1)
            lossG = criterion(output, label)
            lossG.backward()
            optimizerG.step()
            D_G_z2 = output.mean().item()  # 临时保存 D(G(z))-对错误的数据的判别结果

            running_g_loss += lossG.item()
            running_d_loss += lossD.item()

            # Output training stats
            sys_flush_log('Epoch[{}/{}] {:.2f}%'.format(epoch+1, args.epochs, (i+1)/len(dataloader)*100))

            # Check how the generator is doing by saving G's output on fixed_noise
            if (iters % plt_iters == plt_iters - 1) or ((epoch == args.epochs - 1) and (i == len(dataloader) - 1)):
                with torch.no_grad():
                    fake = G(fixed_noise).detach().cpu()
                    plt.axis("off")
                    plt.title("Fake Images Generated by Fixed Noise")
                    plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True), [1, 2, 0]))
                    plt.savefig(trainer.model_dir + "/fake_image.jpg")
                    plt.close()
            iters += 1

        G_losses.append(running_g_loss / len(dataloader))
        D_losses.append(running_d_loss / len(dataloader))

        print_and_log(' > D-Loss:{:.3f}, G-Loss:{:.3f}'.format(G_losses[-1], D_losses[-1]))
        save_pic(trainer.model_dir, {
            "Generator Loss": G_losses,
            "Discriminator Loss": D_losses
        })
